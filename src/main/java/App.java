import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.model.ObjectListing;
import com.amazonaws.services.s3.model.S3Object;
import com.amazonaws.services.s3.model.S3ObjectSummary;
import com.google.auth.oauth2.GoogleCredentials;
import com.google.cloud.bigquery.BigQuery;
import com.google.cloud.bigquery.BigQueryOptions;
import com.google.cloud.bigquery.Field;
import com.google.cloud.bigquery.FormatOptions;
import com.google.cloud.bigquery.Job;
import com.google.cloud.bigquery.JobId;
import com.google.cloud.bigquery.JobStatistics;
import com.google.cloud.bigquery.LegacySQLTypeName;
import com.google.cloud.bigquery.Schema;
import com.google.cloud.bigquery.StandardTableDefinition;
import com.google.cloud.bigquery.Table;
import com.google.cloud.bigquery.TableDataWriteChannel;
import com.google.cloud.bigquery.TableDefinition;
import com.google.cloud.bigquery.TableId;
import com.google.cloud.bigquery.TableInfo;
import com.google.cloud.bigquery.WriteChannelConfiguration;
import com.google.common.collect.Lists;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.OutputStream;
import java.nio.channels.Channels;
import java.nio.charset.Charset;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.StandardCopyOption;
import java.util.ArrayList;
import java.util.List;
import java.util.stream.Collectors;
import org.apache.commons.csv.CSVFormat;
import org.apache.commons.csv.CSVParser;
import org.apache.commons.csv.CSVRecord;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/*
 * This Java source file was generated by the Gradle 'init' task.
 */
public class App {
    protected static final Logger log = LoggerFactory.getLogger(App.class);

    public static void main(String[] args) {
        S3Config s3Config = new S3Config();
        List<String> tablesExported = exportRedshift(s3Config);

        String googleCredentialsPath = "/Users/rfellows/bigquery/bigquery.creds.json";
        try {
            importBigQuery(s3Config, googleCredentialsPath, tablesExported);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    private static List<String> exportRedshift(S3Config s3Config) {
        String tenant = "slapshot";
        String datasetId = "c31fcac5-fb77-4703-9f2d-3fbe27b03c8c";
        String prefix = tenant + "-" + datasetId;
        String schema = "admin";
        RedshiftConfig config = new RedshiftConfig("alpha.ckbaoeftsvvz.us-west-2.redshift.amazonaws.com",
                5439,"alpha");
        RedshiftUtils redshift = new RedshiftUtils(config, s3Config);

        List<String> tableNames = redshift.getTableNames(schema, prefix);

        List<String> tables = tableNames.stream()
                .filter(t -> {
//                    if (t.equals(prefix) || t.startsWith(prefix + "_trgt_")) {
//                        return true;
//                    } else {
                    if (t.startsWith(prefix + "_currentrecord")) {
                        return true;
                    } else {
                        return false;
                    }
                })
                .collect(Collectors.toList());

        for (String tableName : tables) {
            log.info("UNLOADING {}", tableName);
            redshift.unloadTableDefinition(tableName, schema);
            redshift.unloadTableToS3(tableName);
        }
        return tables;
    }

    private static void importBigQuery(S3Config s3Config, String googleCredntialsPath, List<String> tableNames) throws IOException {
        // get objects from s3
        AmazonS3 amazonS3 = Utils.getS3(s3Config);

        GoogleCredentials creds = GoogleCredentials.fromStream(new FileInputStream(googleCredntialsPath))
                .createScoped(Lists.newArrayList("https://www.googleapis.com/auth/cloud-platform"));

        ObjectListing objectListing = amazonS3.listObjects(s3Config.getBucket(), s3Config.getPrefix());

        List<String> tableKeys = objectListing.getObjectSummaries()
                .stream()
                .map(S3ObjectSummary::getKey)
                .filter(path -> !path.contains("/data/"))
                .filter(path -> {
                    if (!tableNames.isEmpty()) {
                        return tableNames.stream().anyMatch(tn -> path.contains("/" + tn + "/"));
                    } else {
                        return true;
                    }
                })
                .collect(Collectors.toList());

        for (String table : tableKeys) {
            log.info("IMPORTING tableKey {}", table);
            importTable(s3Config, amazonS3, creds, table);
        }
    }

    private static void importTable(S3Config s3Config, AmazonS3 amazonS3, GoogleCredentials creds, String table) {
        BigQuery bigquery = BigQueryOptions.newBuilder().setCredentials(creds).build().getService();

        String tableName = table.substring(s3Config.getPrefix().length() + 1, table.length() - 1 - "metadata.csv".length());
        log.info("Creating BigQuery table for {}", tableName);

        S3Object metadata = amazonS3.getObject(s3Config.getBucket(), table);
        try {
            TableId tableId = TableId.of("my-project-1483640368617", "slapshot", safeName(tableName));

            CSVParser records = CSVParser.parse(metadata.getObjectContent(), Charset.forName("UTF-8"),
                    CSVFormat.DEFAULT.withDelimiter(',').withFirstRecordAsHeader().withQuote('"').withRecordSeparator("\n"));
            String columnName;
            String columnType;
            List<Field> fields = new ArrayList<>();

            for (CSVRecord record : records) {
                columnName = record.get(0);
                columnType = record.get(1);

                Field field = Field.of(safeName(columnName), getBigQueryType(columnType));
                fields.add(field);
            }

            metadata.getObjectContent().close();

            Table tableExists = bigquery.getTable(tableId);

            if (tableExists != null) {
                // delete it
                log.info("{} already exists, deleting it", safeName(tableName));
                bigquery.delete(tableId);
            }

            Schema schema = Schema.of(fields);
            TableDefinition tableDefinition = StandardTableDefinition.of(schema);
            TableInfo tableInfo = TableInfo.newBuilder(tableId, tableDefinition).build();
            Table bqTable = bigquery.create(tableInfo);
            log.info("Created table {} with {} columns", bqTable.getTableId().getTable(), fields.size());

            // load the data
            loadData(amazonS3, s3Config, table, bigquery, tableId);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    private static void loadData(AmazonS3 s3, S3Config config, String table, BigQuery bigquery, TableId tableId) {
        String dataObjectKey = table.substring(0, table.indexOf("metadata.csv")) + "data";
        ObjectListing objectListing = s3.listObjects(config.getBucket(), dataObjectKey);
        List<String> dataFileKeys = objectListing.getObjectSummaries().stream().map(S3ObjectSummary::getKey).collect(Collectors.toList());
        for (String dataFileKey : dataFileKeys) {
            String partFile = dataFileKey.substring(dataObjectKey.length() + 1);

            log.info("Loading data from {}", dataFileKey);
            S3Object f = s3.getObject(config.getBucket(), dataFileKey);
            if (f.getObjectContent() != null) {
                WriteChannelConfiguration writeChannelConfiguration =
                        WriteChannelConfiguration.newBuilder(tableId)
                                .setFormatOptions(FormatOptions.csv().toBuilder().setFieldDelimiter("|").build())
                                .build();

                JobId jobId = JobId.newBuilder()
                        .setProject(tableId.getProject())
                        .setRandomJob()
                        .setLocation("US")
                        .build();

                TableDataWriteChannel writer = bigquery.writer(jobId, writeChannelConfiguration);
                // Write data to writer
                try (OutputStream stream = Channels.newOutputStream(writer)) {
                    Path tempFile = Files.createTempFile(tableId.getTable(), "");
                    log.info("Writing temp file {}", tempFile.toString());
                    Files.copy(f.getObjectContent(), tempFile, StandardCopyOption.REPLACE_EXISTING);

                    log.info("Loading bigquery table {}", tableId.getTable());
                    Files.copy(tempFile, stream);
                    tempFile.toFile().deleteOnExit();
                } catch (IOException e) {
                    e.printStackTrace();
                } finally {
                    try {
                        f.getObjectContent().close();
                    } catch (IOException e) {
                        e.printStackTrace();
                    }
                }

                // Get load job
                Job job = writer.getJob();
                if (job != null) {
                    try {
                        job = job.waitFor();
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                    JobStatistics.LoadStatistics stats = job.getStatistics();
                    log.info(stats.toString());
                }
            }
        }
    }

    private static String safeName (String name) {
        return name.replaceAll("[^A-Za-z0-9_]", "_");
    }

    private static LegacySQLTypeName getBigQueryType(String redshiftType) {
        if (redshiftType.startsWith("character varying")) {
            return LegacySQLTypeName.STRING;
        } else if (redshiftType.equalsIgnoreCase("boolean")) {
            return LegacySQLTypeName.BOOLEAN;
        } else if (redshiftType.startsWith("numeric")) {
            return LegacySQLTypeName.NUMERIC;
        } else if (redshiftType.equalsIgnoreCase("timestamp without time zone")) {
            return LegacySQLTypeName.TIMESTAMP;
        } else if (redshiftType.equalsIgnoreCase("integer")) {
            return LegacySQLTypeName.INTEGER;
        } else {
            log.warn("UNKNOWN type for {}, using STRING", redshiftType);
            return LegacySQLTypeName.STRING;
        }
    }
}
